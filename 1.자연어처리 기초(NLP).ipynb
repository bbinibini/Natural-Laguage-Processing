{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "이미 파이썬에서 여러 연산을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'pain', 'no', 'gain']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'No pain no gain'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split().index('gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niap'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[1][::-1] #슬라이싱 후 뒤집기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"유니코드 지원해서 한글도 처리 가능\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'유니코드'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어처리\n",
    "문화적 특성에 따라 언어가 생성되기때문에, 처리하는 방법이 다름.\n",
    "별도로 구분해서 생각해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대소문자 통합\n",
    "* 대소문자를 통합하지 않는다면 컴퓨터는 같은 단어를 다르게 받아들임\n",
    "* 파이썬의 내장 함수 lower(), upper()를 통해 통합 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefgh ABCDEFGH\n"
     ]
    }
   ],
   "source": [
    "s = 'AbCdEFgh'\n",
    "str_lower = s.lower()\n",
    "str_upper = s.upper()\n",
    "print(str_lower, str_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I visitied UK from US on 22-09-20\"\n",
    "# 변환가정을 통일성있게 가져야될 필요가 있음\n",
    "# 통일된 이름으로 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s = s.replace(\"UK\", \"United Kingdom\").replace(\"US\", \"United States\").replace(\"-20\",'-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visitied United Kingdom from United States on 22-09-2020'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_s #특정 스트링에 대해 전환과정을 거쳐야될 필요가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규표현식(Regular Expression)\n",
    "* 특정 문자들을 편리하게 지정하고, 추가, 삭제\n",
    "* 데이터 전처리에서 정규 표현식 사용\n",
    "* 파이썬은 re 패키지 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|특수문자|설명|\n",
    "|----------|------------------------------------------|\n",
    "|.|문자 1개를 표현|\n",
    "|?|문자 0개 또는 1개|\n",
    "|* |앞의 문자가 0개 이상|\n",
    "|+|앞의 문자가 최소 1개 이상|\n",
    "|^|뒤의 문자로 문자열이 시작|\n",
    "|\\\\$|앞의 문자로 문자열이 끝남|\n",
    "|\\\\{n\\\\}|n번만큼 반복|\n",
    "|\\\\ {n1,n2 \\\\}|n1이상, n2이하만큼 반복|\n",
    "|\\\\ [abc \\\\]|앞의 문자들 중 한개의 문자와 매치.a-z|\n",
    "|\\\\ [ ^a \\\\]| 해당 문자를 제외하고 매치|\n",
    "|alb| a 또는 b를 나타냄|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정규 표현식에 자주 사용하는 역슬래시(\\\\)를 이용한 문자 규칙\n",
    "\n",
    "| 문자 | 설명 |\n",
    "| - | - |\n",
    "| `\\\\` | 역슬래시 자체를 의미 |\n",
    "| `\\d` | 모든 숫자를 의미, [0-9]와 동일 |\n",
    "| `\\D` | 숫자를 제외한 모든 문자를 의미, [^0-9]와 동일 |\n",
    "| `\\s` | 공백을 의미, [ \\t\\n\\r\\f\\v]와 동일|\n",
    "| `\\S` | 공백을 제외한 모든 문자를 의미, [^ \\t\\n\\r\\f\\v]와 동일 |\n",
    "| `\\w` | 문자와 숫자를 의미, [a-zA-Z0-9]와 동일 |\n",
    "| `\\W` | 문자와 숫자를 제외한 다른 문자를 의미, [^a-zA-Z0-9]와 동일 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match\n",
    "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = 'ab.' #.이라는 곳에 문자 하나만 오면 됨\n",
    "print(re.match(check,'abc'))\n",
    "print(re.match(check,'c'))\n",
    "print(re.match(check,'ab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile\n",
    "* compile을 사용하면 여러번 사용시 일반 사용보다 더 빠른 속도를 보임\n",
    "* re가 아닌 컴파일한 객체 이름을 통해 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re 사용시 소요 시간 : 0.0010061264038085938\n",
      "compile 사용시 소요 시간 : 0.0009887218475341797\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "normal_s_time = time.time()\n",
    "r = 'ab.'\n",
    "for i in range(1000):\n",
    "    re.match(r, 'abc')\n",
    "print('re 사용시 소요 시간 :', time.time() - normal_s_time)\n",
    "\n",
    "compile_s_time = time.time()\n",
    "r = re.compile('ab.')\n",
    "for i in range(1000) :\n",
    "    r.match('abc')\n",
    "print('compile 사용시 소요 시간 :', time.time() - compile_s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search\n",
    "* match와 다르게, search는 문자열의 전체를 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "check = 'ab?'\n",
    "\n",
    "print(re.search('a',check))\n",
    "print(re.match('kkkab',check))\n",
    "print(re.search('kkkab',check))\n",
    "print(re.match('ab',check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split\n",
    "* 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abbc', 'abcbab']\n",
      "['ab', ' abb', ' ab', 'bab']\n",
      "['s', 'acd', '', '', '', 'dsdf', 'dsd', '']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(' ')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('c')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('[1-9]')\n",
    "print(r.split('s1acd2341dsdf1dsd1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub\n",
    "* 정규 표현식과 일치하는 부분을 다른 부분으로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abc defg\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[a-z]', 'abcd', '1')) #흠\n",
    "\n",
    "print(re.sub('[^a-z]', 'abc defg', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### findall\n",
    "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자열을 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('[\\d]', 'lab 2cd 3ef 4g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finditer\n",
    "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자열을 iterator 객체로 반환\n",
    "* iterator 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x0000019A57B81EC8>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n",
      "<re.Match object; span=(12, 13), match='4'>\n"
     ]
    }
   ],
   "source": [
    "iter1 = re.finditer('[\\d]', 'lab 2cd 3ef 4g')\n",
    "print(iter1)\n",
    "for i in iter1 : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)\n",
    "* 특수문자에 대한 처리\n",
    "    * 별도의 처리가 필요\n",
    "    * 일괄적으로 제거하는 방법도 있지만, 특별한 의미를 가질수도 있음\n",
    "    * 일괄적인 제거보다는 데이터의 특성을 파악하고 처리  \n",
    "<br>\n",
    "* 특정 단어에 대한 토큰 분리 방법\n",
    "    * 한 단어지만 토큰으로 분리할때 we're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요\n",
    "    * we're는 한 단어이나 분리해도 별 영향을 끼치지 않지만 United Kingdom은 두 단어가 모여 특정 의미를 가리켜 분리하면 안됨\n",
    "    * 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화\n",
    "* split활용해 공백을 기준으로 단어를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Time is gold'\n",
    "tokens = [x for x in sentence.split(' ')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토큰화는 nltk 패키지의 tokenize 모듈을 사용해 구현 가능\n",
    "* 단어 토큰화는 word_tokenize() 함수를 사용해 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "* 줄바꿈 문자를 기준으로 문장을 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'The world is a beautiful. \\n But of little use to him who cannot read it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful. ', ' But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [x for x in sentences.split('\\n')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nlkt의 sent_tokenize() 함수사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful.', 'But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 구현할 수 있음\n",
    "* 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더 좋은 토큰화를 구현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규 표현식을 이용한 토큰화\n",
    "* 정규표현식을 이용해 간단하게 구현 가능\n",
    "* nltk 패키지는 정규표현식을 사용하는 토큰화 도구인 RegexpTokenizer 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
    "# s는 공백을 의미 - 특수문자를 남김\n",
    "# gaps=True 해당 정규 표현식을 토큰으로 나누기 위한 기준으로 사용한다는 의미\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기타 토크나이저\n",
    "\n",
    "* WhiteSpaceTokenizer : 공백을 기준으로 토큰화\n",
    "* WordPunktTokenizer : 텍스트를 알파벳 문자,숫자, 문자 리스트로 토큰화\n",
    "* MWETokneizer : Multi-Word-Expression의 약자로 'republic of korea'와 같이 여러 단어로 이루어진 특정 그룹을 한 개체로 취급\n",
    "* TweetTokenizer : 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram 추출\n",
    "\n",
    "* n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
    "* n=1일때는 unigram, n=2 bigram, b=3 trigram으로 불림\n",
    "* bigram을 제일 많이 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'royal'),\n",
       " ('royal', 'road'),\n",
       " ('road', 'to'),\n",
       " ('to', 'learning')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'There is no royal road to learning'\n",
    "bigram = list(ngrams(sentence.split(),2))\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is', 'no'),\n",
       " ('is', 'no', 'royal'),\n",
       " ('no', 'royal', 'road'),\n",
       " ('royal', 'road', 'to'),\n",
       " ('road', 'to', 'learning')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram = list(ngrams(sentence.split(),3))\n",
    "trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is']),\n",
       " WordList(['is', 'no']),\n",
       " WordList(['no', 'royal']),\n",
       " WordList(['royal', 'road']),\n",
       " WordList(['road', 'to']),\n",
       " WordList(['to', 'learning'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.ngrams(n=2) #default 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS(Parts of Speech) 태깅\n",
    "* PoS는 품사를 의미\n",
    "* PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think', 'like', 'man', 'of', 'action', 'and', 'act', 'like', 'man']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize('Think like man of action and act like man')\n",
    "words #토크나이즈 해준 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words) #품사의 의미의 태그가 붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gathers', 'NNS'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no moss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS 태그리스트\n",
    "| Number | Tag | Description | 설명 |\n",
    "| -- | -- | -- | -- |\n",
    "| 1 | `CC` | Coordinating conjunction |\n",
    "| 2 | `CD` | Cardinal number |\n",
    "| 3 | `DT` | Determiner | 한정사\n",
    "| 4 | `EX` | Existential there |\n",
    "| 5 | `FW` | Foreign word | 외래어 |\n",
    "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
    "| 7 | `JJ` | Adjective | 형용사 |\n",
    "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
    "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
    "| 10 | `LS` | List item marker |\n",
    "| 11 | `MD` | Modal |\n",
    "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
    "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
    "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
    "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
    "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
    "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
    "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
    "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
    "| 20 | `RB` | Adverb | 부사 |\n",
    "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
    "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
    "| 23 | `RP` | Particle |\n",
    "| 24 | `SYM` | Symbol | 기호\n",
    "| 25 | `TO` | to |\n",
    "| 26 | `UH` | Interjection | 감탄사 |\n",
    "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
    "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
    "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
    "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
    "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
    "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
    "| 33 | `WDT` | Wh-determiner |\n",
    "| 34 | `WP` | Wh-pronoun |\n",
    "| 35 | `WP$` | Possessive wh-pronoun |\n",
    "| 36 | `WRB` | Wh-adverb |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거\n",
    "* 영어의 전치사(on, in), 한국어의 조사(을,를)등은 분석에 필요하지 않은 경우가 많음\n",
    "* 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 형향을 주지 않음\n",
    "* 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완벽하게 제거되지는 않음\n",
    "* 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음\n",
    "* 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'in', 'the']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = \"on in the\"\n",
    "stop_words = stop_words.split(' ')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'stage']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'singer on the stage'\n",
    "sentence = sentence.split(' ')\n",
    "\n",
    "nouns = []\n",
    "for noun in sentence : \n",
    "    if noun not in stop_words :\n",
    "        nouns.append(noun) #불용어사전에 없는 것만 추출\n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nltk 패키지에 불용어 리스트 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tommorow']\n"
     ]
    }
   ],
   "source": [
    "s = \"If you do not walk today, you will have to run tommorow\"\n",
    "words = word_tokenize(s)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'walk', 'today', ',', 'run', 'tommorow']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "for w in words :\n",
    "    if w not in stop_words :\n",
    "        no_stopwords.append(w)\n",
    "\n",
    "print(no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 철자 교정\n",
    "* 텍스트에 오탈자가 존재하는 경우가 있음\n",
    "* 철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621854 sha256=558b8a61a8a798783580b50dbeba44e8c52255fa12f8d3fcc06427cab64bb5d7\n",
      "  Stored in directory: c:\\users\\bini\\appdata\\local\\pip\\cache\\wheels\\3d\\8e\\bd\\f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\bini\\anaconda3\\envs\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install autocorrect\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "spell = Speller('en')\n",
    "print(spell('peoplle'))\n",
    "print(spell('peopple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earlly', 'bird', 'catchess', 'the', 'womm']\n",
      "Early bird catches the worm\n"
     ]
    }
   ],
   "source": [
    "s = word_tokenize(\"Earlly bird catchess the womm\")\n",
    "print(s)\n",
    "ss = ' '.join([spell(s) for s in s])\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어의 단수화와 복수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'bananas', 'oranges']\n",
      "['apple', 'banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "words = 'apples bananas oranges'\n",
    "tb = TextBlob(words)\n",
    "\n",
    "print(tb.words)\n",
    "print(tb.words.singularize()) #단수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'train', 'airplane']\n",
      "['cars', 'trains', 'airplanes']\n"
     ]
    }
   ],
   "source": [
    "words = 'car train airplane'\n",
    "tb = TextBlob(words)\n",
    "\n",
    "print(tb.words)\n",
    "print(tb.words.pluralize()) #복수화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어간(Stemming) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'applic'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('application') #어간만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'begin'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('beginning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('catches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표제어(Lemmatization) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#어간만이 아니라, 의미가 있는 표제어로 추출\n",
    "lematizer.lemmatize('application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beggining'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizer.lemmatize('beggining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizer.lemmatize('catches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개체명 인식(Named Entitiy Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 국가명, 도시명, 어떤 이름을 뽑아줘야 한다.\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome was not built in a day\n"
     ]
    }
   ],
   "source": [
    "s = \"Rome was not built in a day\" #Rome이 개체명\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(word_tokenize(s))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.ne_chunk(tags, binary=True)\n",
    "print(entities) #NE = Named Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 중의성(Lexical Ambiguity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'bats']\n",
      "Synset('saw.v.01')\n",
      "Synset('squash_racket.n.01')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "s = \"I saw bats\" #방망이를 잘랐다 / 박쥐를 봤다 / 방망이를 봤다\n",
    "\n",
    "print(word_tokenize(s))\n",
    "print(lesk(word_tokenize(s), 'saw'))\n",
    "print(lesk(word_tokenize(s), 'bats')) #야구방망이를 보았다라는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "# 한국어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규표현식\n",
    "* 대부분 영어 정규 표현식과 같음\n",
    "* 자음과 모음이 분리되어 있기 때문에, 문법을 지정할 때는 자음과 모음을 동시에 고려해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match\n",
    "* 컴파일한 정규표현식을 이용해 문자열이 정규 표혀식과 맞는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='ㅎ'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "check = '[ㄱ-ㅎ]+'\n",
    "print(re.match(check, 'ㅎ 안녕하세요.'))\n",
    "print(re.match(check, '안녕하세요. ㅎ')) #처음부터 매칭이 안돼서 None으로 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search\n",
    "* match와 다르게, search는 문자열의 전체를 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='ㄱ'>\n",
      "None\n",
      "<re.Match object; span=(2, 3), match='ㄱ'>\n"
     ]
    }
   ],
   "source": [
    "check = '[ㄱ-ㅎ|ㅏ-ㅣ]+'\n",
    "\n",
    "print(re.search(check, \"ㄱ 안녕하세요.\"))\n",
    "print(re.match(check, '안 ㄱ ㅏ '))\n",
    "print(re.search(check, '안 ㄱ ㅏ '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub\n",
    "* 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "가나다라마바사\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[가-힣]', '가나다라마바사', '1'))\n",
    "print(re.sub('[^가-힣]', '가나다라마바사', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)\n",
    "* 한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되는 경우가 많아 지켜지지 않을 가능성 존재\n",
    "* 띄어쓰기가 안되면 정상적인 토큰 분리가 어려움\n",
    "* 형태소라는 개념이 존재해 추가로 고려해주어야 함\n",
    "* '그는' '그가' 같은 의미를 가리키게 처리를 해줘야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "* 한국어는 단어를 분리해도 조사, 접속사 등이 남아 분석에 어려움이 있음\n",
    "* 한국어 토큰화를 위해선 mecab 라이브러리 필요\n",
    "\n",
    "### 한국어 자연어 처리 konlpy와 형태소 분석기 MeCab 설치\n",
    "* https://hong-yp-ml-records.tistory.com/91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab \n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"아버지가방에들어가신다\"\n",
    "mecab.pos(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토큰화만 실행할 때는 tagger.morphs() 라는 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.morphs(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 형태소만 사용하고 싶을때는 tagger.nouns()라는 함수를 이용해 조사, 접속사 등을 제거 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '방']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.nouns(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 문장을 토큰화할때는 kss(korean sentence splitter) 라이브러리 이용\n",
    "* 라이브러리를 이용해도 한국어에는 전치 표현이 존재해 제대로 토큰화가 안됨\n",
    "* 좀 더 나은 학습을 위해 해당 부분을 따로 처리해주어야만 함 (결국 수동..?) ㅠㅠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['진짜? 내일 뭐하지.', '이렇게 애매모호한 문장도? 밥은 먹었어?', '나는...']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "text = \"진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...\"\n",
    "print(kss.split_sentences(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', '저는', '자연어', '처리', '을', '배우고', '있습니다']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = \"안녕하세요 ㅋㅋ 저는 자연어 처리 (Natural language Processing)을 !! 배우고 있습니다.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요 ', ' 저는 자연어 처리 (Natural language Processing)을 !! 배우고 있습니다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[ㄱ-ㅎ]+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "sentence = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 2 2 1 1]]\n",
      "{'think': 7, 'lie': 3, 'man': 5, 'of': 6, 'action': 1, 'and': 2, 'act': 0, 'like': 4, 'thought': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Think lie a man of action and act like man of thought.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 2 1 1]]\n",
      "{'think': 5, 'lie': 2, 'man': 4, 'action': 1, 'act': 0, 'like': 3, 'thought': 6}\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer(stop_words = 'english') #불용어 제거\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray()) \n",
    "print(vector.vocabulary_) #of 빠짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1 1 1 1]]\n",
      "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray()) \n",
    "print(vector.vocabulary_)\n",
    "\n",
    "# 한국어는 이런식으로 띄어쓰기를 기준으로 하면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['평생', '살', '것', '처럼', '꿈', '을', '꾸', '어라', '그리고', '내일', '죽', '을', '것', '처럼', '오늘', '을', '살', '아라']\n",
      "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n",
      "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "corpus = \"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"\n",
    "tokens = tagger.morphs(re.sub(\"(\\.)\", \"\", corpus))\n",
    "\n",
    "vocab = {}\n",
    "bow = []\n",
    "\n",
    "for tok in tokens : \n",
    "    if tok not in vocab.keys() :\n",
    "        vocab[tok] = len(vocab)\n",
    "        bow.insert(len(vocab)-1, 1)\n",
    "    else :\n",
    "        index = vocab.get(tok)\n",
    "        bow[index] = bow[index]+1\n",
    "        \n",
    "print(tokens) \n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 단어 행렬(DTM)\n",
    "* 문서 단어 행렬 (Document - Term - Matrix)은 문서에 등장하는 여러 단어들의 빈도를 행렬로 표현\n",
    "* 각 문서에 대한 BoW를 하나의 행렬로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Think like a man of action and act like man of thought.\",\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
