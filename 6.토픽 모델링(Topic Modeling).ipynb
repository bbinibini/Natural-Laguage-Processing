{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링(Topic Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토픽 모델링은 문서 집합에서 `주제`를 찾아내기 위한 기술\n",
    "* 토픽 모델링은 '`특정 주제에 관한 문서에서는 특정 단어가 자주 등장할 것이다'라는 직관을 기반\n",
    "* 예를 들어, 주제가 '개'인 문서에서는 개의 품종, 개의 특성을 나타내는 단어가 다른 문서에 비해 많이 등장\n",
    "* 주로 사용되는 토픽 모델링 방법은 잠재 의미 분석과 잠재 디리클레 할당 기법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 잠재 의미 분석(Latent Semantic Analysis)\n",
    "\n",
    "* 더 성능이 좋음\n",
    "* 잠재 의미 분석(LSA)은 주로 문서 색인의 의미 검색에 사용\n",
    "* 잠재 의미 인덱싱(Latent Semantic Indexing, LSI)로도 알려져 있음\n",
    "* LSA의 목표는 문서와 단어의 기반이 되는 `잠재적인 토픽`을 발견하는 것\n",
    "* 잠재적인 토픽은 `문서에 있는 단어들의 분포`를 주도한다고 가정\n",
    "\n",
    "* LSA 방법\n",
    "  + 문서 모음에서 생성한 `문서-단어 행렬(Document Term Matrix)`에서 `단어-토픽 행렬(Term-Topic Matrix)`과 `토픽-중요도 행렬(Topic-Importance Matrix)`, 그리고 `토픽-문서 행렬(Topic-Document Matrix)`로 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 잠재 디리클레 할당(Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 잠재 디레클레 할당(LDA)은 대표적인 토픽 모델링  알고리즘 중 하나\n",
    "\n",
    "* 잠재 디레클레 할당 방법\n",
    "  1. 사용자가 토픽이 `개수를 지정`해 알고리즘에 전달\n",
    "  2. `모든 단어들을 토픽 중 하나에 할당`\n",
    "  3. 모든 문서의 모든 단어에 대해 단어 w가 가정에 의거, `$p(t|d)$, $p(w|t)$에 따라 토픽을 재할당`, 이를 반복, 이 때 가정은 자신만이 잘못된 토픽에 할당되어 있고 다른 모든 단어는 올바른 토픽에 할당된다는 것을 의미    \n",
    "\n",
    "* $p(t|d)$ - 문서 d의 단어들 중 토픽 t에 해당하는 비율\n",
    "* 해당 문서의 자주 등장하는 다른 단어의 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미    \n",
    "\n",
    "* $p(w|t)$- 단어 w를 가지고 있는 모든 문서들 중  토픽 t가 할당된 비율\n",
    "* 다른 문서에서 단어 w에 많이 할당된 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                            remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents))\n",
    "documents[0]\n",
    "#텍스트는 개행문자, 특수문자 등 정제가 안되어있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제\n",
    "\n",
    "def clean_text(d) :\n",
    "    pattern = r'[^a-zA-Z\\s]' #알파벳만\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return d\n",
    "\n",
    "def clean_stopwords(d) :\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w.lower() for w in d.split() if w.lower() not in stop_words and len(w) > 3])\n",
    "\n",
    "def preprocessing(d) :\n",
    "    return preprocess_string(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "news_df = pd.DataFrame({'article' : documents})\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096\n"
     ]
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace =True) #NaN으로 변환\n",
    "#news_df.isnull().values.any() #isnull인것있나확인\n",
    "news_df.dropna(inplace=True) #null값 제외\n",
    "print(len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well i'm not sure about the story nad it did s...\n",
       "1        \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
       "2        Although I realize that principle is not one o...\n",
       "3        Notwithstanding all the legitimate fuss about ...\n",
       "4        Well, I will have to change the scoring on my ...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310                                                   \\n\n",
       "11311    \\nI agree.  Home runs off Clemens are always m...\n",
       "11312    I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                          ^^^^^^\\n...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "news_df['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure story seem biased. disagree statemen...\n",
       "1        yeah, expect people read faq, etc. actually ac...\n",
       "2        although realize principle strongest points, w...\n",
       "3        notwithstanding legitimate fuss proposal, much...\n",
       "4        well, change scoring playoff pool. unfortunate...\n",
       "                               ...                        \n",
       "11309    danny rubenstein, israeli journalist, speaking...\n",
       "11310                                                     \n",
       "11311    agree. home runs clemens always memorable. kin...\n",
       "11312    used deskjet orange micros grappler system6.0....\n",
       "11313    ^^^^^^ argument murphy. scared hell came last ...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_stopwords)\n",
    "news_df['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sure',\n",
       "  'stori',\n",
       "  'bias',\n",
       "  'disagre',\n",
       "  'statement',\n",
       "  'media',\n",
       "  'ruin',\n",
       "  'israel',\n",
       "  'reput',\n",
       "  'redicul',\n",
       "  'media',\n",
       "  'pro',\n",
       "  'isra',\n",
       "  'media',\n",
       "  'world',\n",
       "  'live',\n",
       "  'europ',\n",
       "  'realiz',\n",
       "  'incid',\n",
       "  'describ',\n",
       "  'letter',\n",
       "  'occur',\n",
       "  'media',\n",
       "  'ignor',\n",
       "  'subsid',\n",
       "  'israel',\n",
       "  'exist',\n",
       "  'european',\n",
       "  'degre',\n",
       "  'think',\n",
       "  'reason',\n",
       "  'report',\n",
       "  'clearli',\n",
       "  'atroc',\n",
       "  'shame',\n",
       "  'austria',\n",
       "  'daili',\n",
       "  'report',\n",
       "  'inhuman',\n",
       "  'act',\n",
       "  'commit',\n",
       "  'isra',\n",
       "  'soldier',\n",
       "  'bless',\n",
       "  'receiv',\n",
       "  'govern',\n",
       "  'make',\n",
       "  'holocaust',\n",
       "  'guilt',\n",
       "  'awai',\n",
       "  'look',\n",
       "  'jew',\n",
       "  'treat',\n",
       "  'race',\n",
       "  'power',\n",
       "  'unfortun'],\n",
       " ['yeah',\n",
       "  'expect',\n",
       "  'peopl',\n",
       "  'read',\n",
       "  'faq',\n",
       "  'actual',\n",
       "  'accept',\n",
       "  'hard',\n",
       "  'atheism',\n",
       "  'need',\n",
       "  'littl',\n",
       "  'leap',\n",
       "  'faith',\n",
       "  'jimmi',\n",
       "  'logic',\n",
       "  'run',\n",
       "  'steam',\n",
       "  'jim',\n",
       "  'sorri',\n",
       "  'piti',\n",
       "  'jim',\n",
       "  'sorri',\n",
       "  'feel',\n",
       "  'denial',\n",
       "  'faith',\n",
       "  'need',\n",
       "  'pretend',\n",
       "  'happili',\n",
       "  'mayb',\n",
       "  'start',\n",
       "  'newsgroup',\n",
       "  'alt',\n",
       "  'atheist',\n",
       "  'hard',\n",
       "  'bummin',\n",
       "  'bye',\n",
       "  'bye',\n",
       "  'jim',\n",
       "  'forget',\n",
       "  'flintston',\n",
       "  'chewabl',\n",
       "  'bake',\n",
       "  'timmon']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing 하면 토큰화된다\n",
    "tokenized_news = news_df['article'].apply(preprocessing)\n",
    "tokenized_news = tokenized_news.to_list()\n",
    "tokenized_news[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['sure', 'stori', 'bias', 'disagre', 'statement', 'media', 'ruin', 'israel', 'reput', 'redicul', 'media', 'pro', 'isra', 'media', 'world', 'live', 'europ', 'realiz', 'incid', 'describ', 'letter', 'occur', 'media', 'ignor', 'subsid', 'israel', 'exist', 'european', 'degre', 'think', 'reason', 'report', 'clearli', 'atroc', 'shame', 'austria', 'daili', 'report', 'inhuman', 'act', 'commit', 'isra', 'soldier', 'bless', 'receiv', 'govern', 'make', 'holocaust', 'guilt', 'awai', 'look', 'jew', 'treat', 'race', 'power', 'unfortun']),\n",
       "       list(['yeah', 'expect', 'peopl', 'read', 'faq', 'actual', 'accept', 'hard', 'atheism', 'need', 'littl', 'leap', 'faith', 'jimmi', 'logic', 'run', 'steam', 'jim', 'sorri', 'piti', 'jim', 'sorri', 'feel', 'denial', 'faith', 'need', 'pretend', 'happili', 'mayb', 'start', 'newsgroup', 'alt', 'atheist', 'hard', 'bummin', 'bye', 'bye', 'jim', 'forget', 'flintston', 'chewabl', 'bake', 'timmon'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#1글자로된 문장은 지움\n",
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis=0)\n",
    "news_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10936"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim을 이용한 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 4), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(news_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in news_texts]\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재의미 분석을 위한 LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.994*\"max\" + 0.069*\"giz\" + 0.068*\"bhj\" + 0.025*\"qax\" + 0.015*\"biz\" + 0.014*\"nrhj\" + 0.014*\"bxn\" + 0.012*\"nui\" + 0.011*\"ghj\" + 0.011*\"zei\"'),\n",
       " (1,\n",
       "  '0.381*\"file\" + 0.193*\"program\" + 0.169*\"edu\" + 0.162*\"imag\" + 0.130*\"avail\" + 0.126*\"output\" + 0.119*\"includ\" + 0.115*\"inform\" + 0.101*\"pub\" + 0.100*\"time\"'),\n",
       " (2,\n",
       "  '-0.408*\"file\" + -0.335*\"output\" + -0.216*\"entri\" + 0.171*\"peopl\" + 0.153*\"know\" + -0.137*\"onam\" + -0.134*\"program\" + 0.131*\"said\" + -0.129*\"printf\" + -0.115*\"char\"'),\n",
       " (3,\n",
       "  '-0.249*\"imag\" + -0.226*\"edu\" + 0.214*\"output\" + 0.165*\"peopl\" + 0.157*\"know\" + 0.155*\"entri\" + 0.153*\"said\" + -0.153*\"avail\" + -0.142*\"jpeg\" + -0.124*\"pub\"'),\n",
       " (4,\n",
       "  '0.549*\"wire\" + 0.223*\"ground\" + -0.214*\"jpeg\" + -0.213*\"file\" + -0.169*\"imag\" + 0.164*\"circuit\" + 0.157*\"outlet\" + 0.139*\"connect\" + 0.129*\"subject\" + 0.126*\"neutral\"'),\n",
       " (5,\n",
       "  '-0.400*\"jpeg\" + -0.345*\"imag\" + 0.276*\"anonym\" + -0.246*\"wire\" + 0.160*\"privaci\" + 0.156*\"internet\" + -0.151*\"color\" + 0.144*\"post\" + 0.125*\"inform\" + 0.123*\"mail\"'),\n",
       " (6,\n",
       "  '0.460*\"file\" + 0.215*\"wire\" + -0.169*\"output\" + -0.158*\"program\" + -0.154*\"edu\" + 0.144*\"anonym\" + 0.141*\"firearm\" + -0.137*\"widget\" + 0.123*\"jpeg\" + -0.120*\"entri\"'),\n",
       " (7,\n",
       "  '0.298*\"anonym\" + -0.286*\"file\" + 0.247*\"imag\" + 0.247*\"jpeg\" + -0.178*\"widget\" + 0.170*\"post\" + 0.166*\"internet\" + 0.155*\"privaci\" + -0.138*\"control\" + -0.127*\"drive\"'),\n",
       " (8,\n",
       "  '-0.295*\"hockei\" + -0.279*\"team\" + -0.235*\"game\" + -0.233*\"leagu\" + -0.188*\"season\" + -0.163*\"year\" + 0.137*\"widget\" + -0.129*\"plai\" + -0.128*\"space\" + 0.122*\"anonym\"'),\n",
       " (9,\n",
       "  '0.500*\"drive\" + 0.275*\"disk\" + 0.202*\"control\" + 0.185*\"hard\" + 0.168*\"bio\" + -0.158*\"edu\" + 0.155*\"support\" + 0.138*\"card\" + 0.134*\"featur\" + 0.119*\"scsi\"'),\n",
       " (10,\n",
       "  '-0.480*\"stephanopoulo\" + -0.329*\"presid\" + 0.237*\"drive\" + 0.187*\"armenian\" + 0.183*\"peopl\" + 0.125*\"disk\" + -0.123*\"packag\" + -0.120*\"go\" + -0.120*\"think\" + 0.102*\"control\"'),\n",
       " (11,\n",
       "  '0.286*\"space\" + -0.266*\"stephanopoulo\" + 0.254*\"launch\" + -0.213*\"drive\" + -0.209*\"edu\" + 0.172*\"satellit\" + -0.123*\"presid\" + 0.114*\"widget\" + -0.113*\"disk\" + -0.109*\"com\"'),\n",
       " (12,\n",
       "  '-0.248*\"edu\" + 0.232*\"widget\" + 0.200*\"jpeg\" + 0.172*\"window\" + -0.170*\"graphic\" + 0.161*\"convert\" + -0.148*\"space\" + -0.147*\"pub\" + 0.136*\"applic\" + 0.133*\"hockei\"'),\n",
       " (13,\n",
       "  '0.265*\"jesu\" + 0.196*\"atheist\" + 0.189*\"edu\" + 0.184*\"christian\" + 0.165*\"believ\" + 0.154*\"god\" + 0.145*\"exist\" + -0.139*\"launch\" + -0.133*\"armenian\" + -0.125*\"space\"'),\n",
       " (14,\n",
       "  '-0.290*\"space\" + -0.218*\"launch\" + -0.176*\"stephanopoulo\" + 0.166*\"program\" + 0.162*\"russian\" + 0.159*\"govern\" + -0.153*\"satellit\" + 0.147*\"administr\" + -0.128*\"jesu\" + -0.123*\"post\"'),\n",
       " (15,\n",
       "  '-0.248*\"imag\" + 0.236*\"com\" + 0.235*\"jpeg\" + 0.233*\"edu\" + -0.224*\"data\" + 0.218*\"launch\" + 0.168*\"space\" + 0.164*\"post\" + -0.162*\"graphic\" + 0.146*\"anonym\"'),\n",
       " (16,\n",
       "  '0.238*\"south\" + 0.219*\"rockefel\" + 0.204*\"secret\" + 0.197*\"island\" + 0.182*\"nuclear\" + 0.166*\"bolshevik\" + -0.157*\"health\" + 0.153*\"militari\" + 0.143*\"ship\" + 0.134*\"georgia\"'),\n",
       " (17,\n",
       "  '-0.255*\"anonym\" + -0.222*\"edu\" + -0.197*\"post\" + 0.181*\"jesu\" + 0.170*\"pub\" + 0.153*\"privaci\" + -0.150*\"imag\" + -0.135*\"health\" + 0.128*\"window\" + 0.126*\"inform\"'),\n",
       " (18,\n",
       "  '0.290*\"stephanopoulo\" + 0.180*\"health\" + -0.179*\"administr\" + -0.161*\"govern\" + -0.155*\"senior\" + -0.147*\"offici\" + -0.147*\"russia\" + -0.143*\"fund\" + -0.143*\"russian\" + -0.123*\"think\"'),\n",
       " (19,\n",
       "  '0.386*\"turkish\" + 0.314*\"jew\" + 0.224*\"armenian\" + 0.202*\"jpeg\" + 0.197*\"turkei\" + 0.154*\"nazi\" + -0.132*\"entri\" + -0.129*\"imag\" + 0.129*\"drive\" + 0.121*\"edu\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi_model = LsiModel(corpus, num_topics=20, id2word=dictionary)\n",
    "topics = lsi_model.print_topics()\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 어떠한 토픽 개수를 가지는게 좋은지를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "min_topics, max_topics = 10,30\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in range(min_topics, max_topics) :\n",
    "    model = LsiModel(corpus, num_topics, id2word=dictionary)\n",
    "    coherence = CoherenceModel(model = model,\n",
    "                              texts = news_texts,\n",
    "                              dictionary=dictionary)\n",
    "    coherence_scores.append(coherence.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.647557495705026,\n",
       " 0.5445653870913344,\n",
       " 0.4951423152227034,\n",
       " 0.5341457749867063,\n",
       " 0.5301868685939864,\n",
       " 0.4808423635086711,\n",
       " 0.5042212792390055,\n",
       " 0.4758292685164439,\n",
       " 0.46455593599429296,\n",
       " 0.47091221074917355,\n",
       " 0.4799178039468475,\n",
       " 0.4876422154291191,\n",
       " 0.4659289254026209,\n",
       " 0.5630740897649661,\n",
       " 0.5196421712532443,\n",
       " 0.4128579942775664,\n",
       " 0.44493610150941443,\n",
       " 0.4880701491578219,\n",
       " 0.43283792472688704,\n",
       " 0.5043724116245221]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotliv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-42b8aa5d5229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotliv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seaborn-white'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotliv'"
     ]
    }
   ],
   "source": [
    "import matplotliv.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "x = [int(i) for i in range(min_topics, max_topics)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, coherence_scores)\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus, num_topics=24, id2word=dictionary)\n",
    "topics = lsi_model.print_topics(num_topics=24)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재 디리클레 할당을 위한 LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
