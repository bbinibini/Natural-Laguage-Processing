{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트처리\n",
    "이미 파이썬에서 여러 연산을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'pain', 'no', 'gain']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'No pain no gain'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split().index('gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niap'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[1][::-1] #슬라이싱 후 뒤집기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"유니코드 지원해서 한글도 처리 가능\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'유니코드'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어처리\n",
    "문화적 특성에 따라 언어가 생성되기때문에, 처리하는 방법이 다름.\n",
    "별도로 구분해서 생각해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대소문자 통합\n",
    "* 대소문자를 통합하지 않는다면 컴퓨터는 같은 단어를 다르게 받아들임\n",
    "* 파이썬의 내장 함수 lower(), upper()를 통해 통합 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefgh ABCDEFGH\n"
     ]
    }
   ],
   "source": [
    "s = 'AbCdEFgh'\n",
    "str_lower = s.lower()\n",
    "str_upper = s.upper()\n",
    "print(str_lower, str_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I visitied UK from US on 22-09-20\"\n",
    "# 변환가정을 통일성있게 가져야될 필요가 있음\n",
    "# 통일된 이름으로 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s = s.replace(\"UK\", \"United Kingdom\").replace(\"US\", \"United States\").replace(\"-20\",'-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visitied United Kingdom from United States on 22-09-2020'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_s #특정 스트링에 대해 전환과정을 거쳐야될 필요가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규표현식(Regular Expression)\n",
    "* 특정 문자들을 편리하게 지정하고, 추가, 삭제\n",
    "* 데이터 전처리에서 정규 표현식 사용\n",
    "* 파이썬은 re 패키지 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|특수문자|설명|\n",
    "|----------|------------------------------------------|\n",
    "|.|문자 1개를 표현|\n",
    "|?|문자 0개 또는 1개|\n",
    "|* |앞의 문자가 0개 이상|\n",
    "|+|앞의 문자가 최소 1개 이상|\n",
    "|^|뒤의 문자로 문자열이 시작|\n",
    "|\\\\$|앞의 문자로 문자열이 끝남|\n",
    "|\\\\{n\\\\}|n번만큼 반복|\n",
    "|\\\\ {n1,n2 \\\\}|n1이상, n2이하만큼 반복|\n",
    "|\\\\ [abc \\\\]|앞의 문자들 중 한개의 문자와 매치.a-z|\n",
    "|\\\\ [ ^a \\\\]| 해당 문자를 제외하고 매치|\n",
    "|alb| a 또는 b를 나타냄|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 역슬래시(\\\\)를 이용한 문자 규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### match\n",
    "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = 'ab.' #.이라는 곳에 문자 하나만 오면 됨\n",
    "print(re.match(check,'abc'))\n",
    "print(re.match(check,'c'))\n",
    "print(re.match(check,'ab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compile\n",
    "* compile을 사용하면 여러번 사용시 일반 사용보다 더 빠른 속도를 보임\n",
    "* re가 아닌 컴파일한 객체 이름을 통해 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re 사용시 소요 시간 : 0.0010061264038085938\n",
      "compile 사용시 소요 시간 : 0.0009887218475341797\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "normal_s_time = time.time()\n",
    "r = 'ab.'\n",
    "for i in range(1000):\n",
    "    re.match(r, 'abc')\n",
    "print('re 사용시 소요 시간 :', time.time() - normal_s_time)\n",
    "\n",
    "compile_s_time = time.time()\n",
    "r = re.compile('ab.')\n",
    "for i in range(1000) :\n",
    "    r.match('abc')\n",
    "print('compile 사용시 소요 시간 :', time.time() - compile_s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### search\n",
    "* match와 다르게, search는 문자열의 전체를 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "check = 'ab?'\n",
    "\n",
    "print(re.search('a',check))\n",
    "print(re.match('kkkab',check))\n",
    "print(re.search('kkkab',check))\n",
    "print(re.match('ab',check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split\n",
    "* 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abbc', 'abcbab']\n",
      "['ab', ' abb', ' ab', 'bab']\n",
      "['s', 'acd', '', '', '', 'dsdf', 'dsd', '']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(' ')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('c')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('[1-9]')\n",
    "print(r.split('s1acd2341dsdf1dsd1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sub\n",
    "* 정규 표현식과 일치하는 부분을 다른 부분으로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abc defg\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[a-z]', 'abcd', '1')) #흠\n",
    "\n",
    "print(re.sub('[^a-z]', 'abc defg', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### findall\n",
    "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자열을 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('[\\d]', 'lab 2cd 3ef 4g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finditer\n",
    "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자열을 iterator 객체로 반환\n",
    "* iterator 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x0000019A57B81EC8>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n",
      "<re.Match object; span=(12, 13), match='4'>\n"
     ]
    }
   ],
   "source": [
    "iter1 = re.finditer('[\\d]', 'lab 2cd 3ef 4g')\n",
    "print(iter1)\n",
    "for i in iter1 : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)\n",
    "* 특수문자에 대한 처리\n",
    "    * 별도의 처리가 필요\n",
    "    * 일괄적으로 제거하는 방법도 있지만, 특별한 의미를 가질수도 있음\n",
    "    * 일괄적인 제거보다는 데이터의 특성을 파악하고 처리  \n",
    "<br>\n",
    "* 특정 단어에 대한 토큰 분리 방법\n",
    "    * 한 단어지만 토큰으로 분리할때 we're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요\n",
    "    * we're는 한 단어이나 분리해도 별 영향을 끼치지 않지만 United Kingdom은 두 단어가 모여 특정 의미를 가리켜 분리하면 안됨\n",
    "    * 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화\n",
    "* split활용해 공백을 기준으로 단어를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Time is gold'\n",
    "tokens = [x for x in sentence.split(' ')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토큰화는 nltk 패키지의 tokenize 모듈을 사용해 구현 가능\n",
    "* 단어 토큰화는 word_tokenize() 함수를 사용해 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화\n",
    "* 줄바꿈 문자를 기준으로 문장을 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'The world is a beautiful. \\n But of little use to him who cannot read it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful. ', ' But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [x for x in sentences.split('\\n')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nlkt의 sent_tokenize() 함수사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful.', 'But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 구현할 수 있음\n",
    "* 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더 좋은 토큰화를 구현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식을 이용한 토큰화\n",
    "* 정규표현식을 이용해 간단하게 구현 가능\n",
    "* nltk 패키지는 정규표현식을 사용하는 토큰화 도구인 RegexpTokenizer 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
    "# s는 공백을 의미 - 특수문자를 남김\n",
    "# gaps=True 해당 정규 표현식을 토큰으로 나누기 위한 기준으로 사용한다는 의미\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기타 토크나이저\n",
    "\n",
    "* WhiteSpaceTokenizer : 공백을 기준으로 토큰화\n",
    "* WordPunktTokenizer : 텍스트를 알파벳 문자,숫자, 문자 리스트로 토큰화\n",
    "* MWETokneizer : Multi-Word-Expression의 약자로 'republic of korea'와 같이 여러 단어로 이루어진 특정 그룹을 한 개체로 취급\n",
    "* TweetTokenizer : 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram 추출\n",
    "\n",
    "* n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
    "* n=1일때는 unigram, n=2 bigram, b=3 trigram으로 불림\n",
    "* bigram을 제일 많이 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'royal'),\n",
       " ('royal', 'road'),\n",
       " ('road', 'to'),\n",
       " ('to', 'learning')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'There is no royal road to learning'\n",
    "bigram = list(ngrams(sentence.split(),2))\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is', 'no'),\n",
       " ('is', 'no', 'royal'),\n",
       " ('no', 'royal', 'road'),\n",
       " ('royal', 'road', 'to'),\n",
       " ('road', 'to', 'learning')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram = list(ngrams(sentence.split(),3))\n",
    "trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is']),\n",
       " WordList(['is', 'no']),\n",
       " WordList(['no', 'royal']),\n",
       " WordList(['royal', 'road']),\n",
       " WordList(['road', 'to']),\n",
       " WordList(['to', 'learning'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.ngrams(n=2) #default 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS(Parts of Speech) 태깅\n",
    "* PoS는 품사를 의미\n",
    "* PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think', 'like', 'man', 'of', 'action', 'and', 'act', 'like', 'man']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize('Think like man of action and act like man')\n",
    "words #토크나이즈 해준 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bini\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words) #품사의 의미의 태그가 붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gathers', 'NNS'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no moss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PoS 태그 리스트"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
